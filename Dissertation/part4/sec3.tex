\section{Методы построения сурогантных моделей}\label{sec:ch4/sec3}
Суррогатное моделирование является достаточно эффектиынвм методом в
задачах многокритериальной оптимизации, особенно при построении
Парето-фронта. Как правило, оно используется для аппроксимации сложных и вычислительно
затратных функций или систем, что позволяет значительно снизить временные и вычислительные
затраты на решение задач, требующих многократной оценки целевых функций. Данный
подход особенно полезен в тех случаях, когда каждая отдельная оценка целевой функции
является дорогостоящей с точки зрения времени или ресурсов, как это часто встречается
при численном моделировании физических процессов или компьютерных симуляциях сложных
инженерных систем.

Для синжения Для снижения вычислительных затрат используется суррогатное моделирование,
при котором сложные функции аппроксимируются с помощью более простых моделей,
называемых суррогатами, которые можно быстро и эффективно вычислять.
Это позволяет оптимизировать процесс поиска Парето-фронта и находить решения
с меньшими затратами ресурсов.



\subsection{Обзор методов суррогатного моделирования}\label{sec:ch4/sec3/subsec1}


\paragraph{Полиномиальная регрессия}\label{sec:ch4/sec3/subsec1/subsubsec1}

Полиномиальная регрессия представляет собой метод аппроксимации данных
с использозванием полиномов различных степеней. Этот метод является расширением линейной
регрессии и позволяет моделировать зависисмости между входными и выходными переменными более
гибким образом, используя дополнительные нелинейные термины, такие как квадратичные, кубические члены
и~т.д., а также их взаимодействия \cite{fan2018local}.

Полиномиальная регрессия предполагает, что зависимость между предикторами и откликом
можеты быть описана полиноминальной функцией вида:

\begin{equation}
    \hat{y}(\mathbf{x}) = \beta_0 + \sum_{i=1}^{p} \beta_i x_i + \sum_{i=1}^{p} \sum_{j=1}^{p} \beta_{ij} x_i x_j + \ldots,
\end{equation}
где $\hat{y}(\mathbf{x})$ -- предсказанное значение отклика;
$\beta_0$ -- свободный член (интерцепт);
$\beta_i$ -- коэффициенты линейных членов $x_i$;
$\beta_{ij}$ -- коэффициенты взаимодействия между переменными $x_i$ и $x_j$;
$p$ -- количество перменных;
$x_i,~x_j$ -- входные переменные.

Для случая квадратичной регрессии полиноминальная функция принимает вид:

\begin{equation}
    \hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2
    + \beta_4 x_2^2 + \beta_5 x_1 x_2.
\end{equation}

Здесь учитываются коэффициенты линейных $\beta_1,~\beta_2$ и квадратичных $\beta_3,~\beta_4$ членов,
а также коэффициент взаимодействия $\beta_5$ \cite{heiberger2009polynomial}.

Для определения коэффициентов $\beta$ используется метод наименьших квадратов,
который минимизирует сумму квадратов отколнений между наблюдаемыми значениям $y_i$
и предсказанными $\hat{y}_i$. Задача оптимизации формулируется следующим образом:

\begin{equation}
    \min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 =
    \min_{\beta} \sum_{i=1}^{n} \left\{
    y_i - \left(\beta_0 + \sum_{j=1}^{p} \beta_j x_{ij}
    + \sum_{j=1}^{p} \sum_{k=1}^{p} \beta_{jk} x_{ij} x_{ik}
    \right)
    \right\}^2,
\end{equation}
где $n$ -- количество наблюдений.

Для решения этой задачи вводится матричная форма:

\begin{equation}
    \mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
\end{equation}
где $\mathbf{y}$ -- вектор наблюдаемых значений откликов размерности $n \times 1$;
$\mathbf{X}$ -- матрица признаков размерности $n \times m$, где $m$ -- количество коэффициентов,
включая взаимодействия и нелинейные члены;
$\boldsymbol{\beta}$ -- вектор коэффициентов размерности $m \times 1$;
$\boldsymbol{\epsilon}$ -- вектор ошибок.

Оценка коэффицикнтов $\boldsymbol{\beta}$ производится с использованием псевдообратной матрицы:
\begin{equation}
    \boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.
\end{equation}
где $(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ -- псевдообратная матрица Мура-Пенроуза,
которая обеспечивает минимизацию ошибки на оценке коэффициентов \cite{meyer2009matrix}.

Преимущества полиномиальной регрессии для задачи оптимизации управления электропневматическими приводами:
\begin{enumerate}
    \item Простота реализации и интерпретации модели.
    \item Низкие вычислительные затраты на построение и использование модели.
    \item Возможность аналитического вычисления градиентов, что полезно для оптимизационных алгоритмов.
\end{enumerate}

Ограничения метода:
\begin{enumerate}
    \item Ограниченная способность моделировать сложные нелинейные зависимости, характерные для пневматических систем;
    \item Риск переобучения при использовании полиномов высоких степеней;
    \item Чувствительность к выбросам в экспериментальных данных.
\end{enumerate}

\paragraph{Радиальные базисные функции}\label{sec:ch4/sec3/subsec1/subsubsec2}


Преимущества метода RBF:

\begin{enumerate}
    \item Способность эффективно аппроксимировать сложные нелинейные зависимости.
    \item Хорошая обобщающая способность при правильном выборе параметров.
    \item Возможность точной интерполяции в экспериментальных точках.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
    \item Чувствительность к выбору параметров (количество и расположение центров, тип базисной функции, параметр формы).
    \item Потенциальные проблемы с обусловленностью матрицы интерполяции при большом количестве базисных функций.
    \item Сложность интерпретации модели по сравнению с полиномиальной регрессией.
\end{enumerate}

\paragraph{Гауссовы процессы (Кригинг)}\label{sec:ch4/sec3/subsec1/subsubsec3}

Кригинг (Гауссовы процессы) является мощным методом интерполяции,
который позволяет строить суррогатные модели для сложных функций,
используя концепцию случайных процессов \cite{gramacy2020surrogates}. Он основан
на предположении, что процесс \( y(\mathbf{x}) \) может быть представлен в виде:

\begin{equation}
    y(\mathbf{x}) = \mu + Z(\mathbf{x}),
\end{equation}
где $\mu$ -- среднее значение; $Z(\mathbf{x})$ -- гауссовский процесс с нулевым средним и
ковариационной функцией \cite{marrel2024probabilistic}:

\begin{equation}
    \text{Cov}(Z(\mathbf{x}_i), Z(\mathbf{x}_j)) = k(\mathbf{x}_i, \mathbf{x}_j),
\end{equation}

Ковариационная функция часто задается как радиальная базисная функция:

\begin{equation}
    k(\mathbf{x}_i, \mathbf{x}_j) = \sigma^2 \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2l^2}\right),
\end{equation}.
где \( \sigma^2 \) — дисперсия;
$l$ — параметр длины \cite{figueroa2021gaussian}.

Предсказание значений в новых точках $\mathbf{x}_*$ осуществляется через условное распределение, учитывающее известные значения.
Среднее предсказание и его дисперсия определяются как:

\begin{equation}
    \hat{y}(\mathbf{x}_*) = \mu + \mathbf{k}_*^T \mathbf{K}^{-1} (\mathbf{y} - \mu \mathbf{1}_n),
\end{equation}

\begin{equation}
    \text{Var}(\hat{y}(\mathbf{x}_*)) = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*,
\end{equation}
где $\mathbf{k}_*$ — вектор ковариаций между новой точкой и обучающими точками;
$\mathbf{K}$ — ковариационная матрица \cite{zhou2020enhanced}.

Кригинг широко применяется в задачах многокритериальной оптимизации и позволяет
не только предсказывать значения, но и оценивать их неопределенность, что особенно
полезно при построении фронтов Парето и выборе компромиссных решений \cite{radaideh2020surrogate}.

Преимущества метода кригинга:

\begin{enumerate}
    \item Высокая точность интерполяции и экстраполяции;
    \item Возможность оценки неопределенности предсказаний;
    \item Гибкость в моделировании сложных нелинейных зависимостей;
    \item Эффективность при ограниченном количестве экспериментальных данных.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
    \item Вычислительная сложность при большом количестве экспериментальных точек;
    \item Чувствительность к выбору функции корреляции и ее параметров;
    \item Сложность интерпретации модели по сравнению с детерминированными методами.
\end{enumerate}

\paragraph{Метод опорных векторов.}

Метод опорных векторов (SVM) представляет собой один из наиболее эффективных методов классификации и
регрессии, основанный на поиске гиперплоскости, которая максимизирует зазор между классами.
Основная идея заключается в преобразовании исходных данных в более
высокое измерение с целью нахождения разделяющей гиперплоскости \cite{Jakkula2006}.

Рассмотрим обучающую выборку:
\begin{equation}
    \{(x_i, y_i)\}_{i=1}^N,
\end{equation}
где $x_i \in \mathbb{R}^n$ -- вектор признаков;
$y_i \in \{-1, 1\}$ -- метка класса.

Задача заключается в нахождении гиперплоскости,
которая разделяет два класса с максимальным зазором. Гиперплоскость определяется уравнением:

\begin{equation}
    f(x) = w^T x + b = 0,
\end{equation}
где $w \in \mathbb{R}^n$ -- вектор весов;
$b \in \mathbb{R}$ -- смещение.

Целью является минимизация следующей функции потерь с учетом ограничений:

\begin{equation}
    \min_{w, b} \frac{1}{2} \|w\|^2,
\end{equation}
при условиях:

\begin{equation}
    y_i (w^T x_i + b) \geq 1, \quad i = 1, \ldots, N.
\end{equation}

Для решения данной задачи применяется метод множителей Лагранжа,
что приводит к следующей двойственной задаче \cite{Patle2013}:

\begin{equation}
    \max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i^T x_j),
\end{equation}
при ограничениях:

\begin{equation}
    \sum_{i=1}^N \alpha_i y_i = 0, \quad \alpha_i \geq 0, \quad i = 1, \ldots, N.
\end{equation}
где $\alpha_i$ -- множители Лагранжа, которые определяют вклад каждого образца в решение задачи.

Для повышения мощности метода используется преобразование исходных данных
в пространство более высокой размерности с помощью ядровых
функций

\begin{equation}
    K(x_i, x_j) = \phi(x_i)^T \phi(x_j),
\end{equation}
где $\phi(\cdot)$ — отображение в новое
пространство признаков.

Распространенные ядра включают линейное,
полиномиальное и гауссово (радиальное базисное) ядро \cite{Deris2011}:

\begin{itemize}
    \item Линейное: $K(x_i, x_j) = x_i^T x_j$;
    \item Полиномиальное: $K(x_i, x_j) = (x_i^T x_j + 1)^d$;
    \item Гауссово: $K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$.
\end{itemize}

Для работы с шумными данными вводится параметр регуляризации $C$,
который контролирует баланс между шириной зазора и ошибками классификации.
Оптимизационная задача в этом случае принимает вид \cite{Boswell2002}:

\begin{equation}
    \min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i,
\end{equation}

при условиях:

\begin{equation}
    y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i = 1, \ldots, N,
\end{equation}
где $\xi_i$ -- переменные, отвечающие за допущенные ошибки.

Преимущества метода опорных векторов: 

\begin{enumerate}
    \item Высокая обобщающая способность, особенно при ограниченном наборе обучающих данных;
    \item Эффективность в задачах с большим количеством входных параметров;
    \item Способность моделировать сложные нелинейные зависимости.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
    \item - Вычислительная сложность обучения модели для больших наборов данных;
    \item Чувствительность к выбору ядерной функции и настройке гиперпараметров;
    \item Сложность интерпретации модели по сравнению с более простыми методами, такими как полиномиальная регрессия.
\end{enumerate}

\paragraph{Нейронные сети}\label{sec:ch4/sec3/subsec1/subsubsec5}

\paragraph{Эволюционные алгоритмы и метаэвристики}\label{sec:ch4/sec3/subsec1/subsubsec6}
Эволюционные алгоритмы и метаэвристики представляют собой группу методов оптимизации,
вдохновленных природными процессами.
Они основанны на принципа биологической эвлюции и предлагают подходы, такие как генетические алгоритмы,
алгоритмы роя частиц, иммунные алгоритмы и др., которые позволяют эффективно искать решения
в пространстве возможных параметров \cite{zhang2015comparision}.

Основная идея эволюционных алгоритмов заключается в имитации процесса естественного отбора,
где популяция решений обновляется с каждым шагом алгоритма, и только наилучшие решения
сохраняются для следующего поколения. Алгоритм начинается с инициализации популяции случайных
решений, где каждое решение представляет собой набор параметров управления. Далее, для каждой
особи в популяции оценивается функция приспособленности, которая определяет, насколько хорошо
решение выполняет поставленную задачу. На основе функции приспособленности отбираются лучшие
решения, которые подвергаются мутации и скрещиванию, чтобы создать новые решения, и процесс
повторяется до тех пор, пока не будет достигнута заданная точность или не исчерпаны
вычислительные ресурсы.

Математически, решение $\mathbf{x}$ оптимизируется с помощью эволюционного алгоритма, следуюя
процедуре:

\begin{equation}
    x^{t+1} = \text{ОТБОР} \left\{
    \text{МУТАЦИЯ} \left\{
    \text{СКРЕЩИВАНИЕ} \left\{
    x^t
    \right\}
    \right\}
    \right\},
\end{equation}
где $x^t$ -- текущее решение;
$x^{t+1}$ -- новое решение;
$\text{ОТБОР}$ -- процедура отбора лучших решений или решений с высокой приспособленностью;
$\text{МУТАЦИЯ}$ -- процедура случайного или направленного изменения решения;
$\text{СКРЕЩИВАНИЕ}$ -- процедура комбинирования решений для создания новых вариантов.

Основные элементы эволюционного алгоритма можно описать следующим образом:

\begin{enumerate}
    \item Инициализация популяции: создание случайной популяции решений $x_i,~i = 1, \ldots, N$;
    \item Оценка приспособленности: для каждого решения рассчитвыается функция приспособленности $f(x_i)$,
          определяющая качество решения;
    \item Отбор: выбор лучших решений для создания нового решения;
    \item Скрещивание и мутация: создаются новые решения путем комбинирования и изменения существующих;
    \item Замена: новые решения заменяют старые в популяции и процесс повторяется до достижения критерия останова.
\end{enumerate}

Основные трудности применения эволюционных алгоритмов заключается
в определении параметров алгоритма, таких как размер популяции, вероятность мутации и
скрещивания, а также в обеспечении достаточной вычислительной мощности для
обработки большого числа итераций.

Метаэврестические подходы, такие как алгоритмы роя частиц и методы имитации отжига,
дополняют эволлюционные алгоритмы, предоставляя дополнительные инструменты для
исследования пространства решений. Эти методы доказали свою эффективность в
решении задач многокритериальной оптимизации, где необходимо сбалансировать
несколько противоречивых показателей качества.

Например, алгоритмы роя частиц (PSO) имитируют поведение стаци, где каждый
агент (частица) премещается в пространтсве решений с учетом своей собственной
истории и информации, полученной от других агентов. Частица $i$ обновляет
свою скорость и положение следующим образом:

\begin{equation}
    \begin{aligned}
        v_i^{t+1} & = \omega v_i^t + c_1 r_1 (p_i^t - x_i^t) + c_2 r_2 (g^t - x_i^t), \\
        x_i^{t+1} & = x_i^t + v_i^{t+1},
    \end{aligned}
\end{equation}
где $\omega$ -- коэффициент инерции;
$c_1, c_2$ -- коэффициенты обучения;
$r_1, r_2$ -- случайные числа;
$p_i$ -- лучшее положение частицы;
$g$ -- лучшее положение частицы.

Преимущества эволюционных алгоритмов и метаэвристик:

\begin{enumerate}
    \item Способность находить компромисные решения в условиях многокритериальной оптимизации;
    \item Эффективность в поиске глобальных оптимумов в пространстве параметров;
    \item Легкость в адаптации для многокритериальной оптимизации, что позволяет учитывать
          различные критерии качества.
\end{enumerate}

Ограничения методов:
\begin{enumerate}
    \item Высокая вычислительная сложность при большом количестве параметров;
    \item Чувствительность к выбору параметров алгоритма;
    \item Нет гарантии нахождения глобального оптимума.
\end{enumerate}

\subsection{Выбор оптимального метода построения суррогатных моделей}\label{sec:ch4/sec3/subsec2}

В рамках исследования методов построения суррогатных моделей
для многокритериальной оптимизации алгоритмов управления
электропневматическими приводами с дискретными распределителями
был применен метод морфологического анализа Фрица Цвикки. Данный
метод позволяет систематически рассмотреть все возможные решения
проблемы путем анализа всех комбинаций параметров, что особенно важно
при выборе оптимального подхода в сложных многопараметрических задачах.

Метод Цвикки включает в себя несколько этапов. На первом этапе
формулируется проблема и определяются ключевые параметры,
характеризующие возможные решения. В нашем случае, ключевыми
параметрами для оценки методов построения суррогатных моделей были выбраны:

A. Способность к аппроксимации нелинейных зависимостей
B. Масштабируемость
C. Вычислительная эффективность
D. Интерпретируемость результатов
E. Способность к обобщению
F. Адаптивность к типам данных
G. Оценка неопределенности

Для каждого параметра были определены возможные значения:
низкое, среднее и высокое (или эквивалентные им).
Это позволяет создать морфологическую матрицу,
которая представляет собой многомерное пространство возможных решений.

\begin{table}[h]
    \centering
    \caption{Морфологическая матрица методов построения суррогатных моделей}
    \begin{tabular}{lccc}
        \midrule
        Параметр                          & Значение 1 & Значение 2 & Значение 3 \\
        \midrule
        A. Способность к аппроксимации    & Низкая     & Средняя    & Высокая    \\
        нелинейных зависимостей           &            &            &            \\

        B. Масштабируемость               & Плохая     & Средняя    & Хорошая    \\
        C. Вычислительная эффективность   & Низкая     & Средняя    & Высокая    \\
        D. Интерпретируемость результатов & Низкая     & Средняя    & Высокая    \\
        E. Способность к обобщению        & Низкая     & Средняя    & Высокая    \\
        F. Адаптивность к типам данных    & Низкая     & Средняя    & Высокая    \\
        G. Оценка неопределенности        & Нет        & Частичная  & Полная     \\
        \midrule
    \end{tabular}
    \label{tab:morphological_matrix}
\end{table}

На следующем этапе анализа каждый рассматриваемый метод
построения суррогатных моделей был оценен по каждому параметру.
Оценка проводилась на основе теоретических свойств методов и опыта
их применения в схожих задачах. Результаты оценки представлены в следующей таблице:

\begin{table}[h]
    \centering
    \caption{Оценка методов по параметрам}
    \begin{tabular}{l|c|c|c|c|c|c|c}
        \midrule
        Метод                             & A & B & C & D & E & F & G \\
        \midrule
        Полиномиальная регрессия          & 1 & 1 & 3 & 3 & 1 & 1 & 1 \\
        \hline
        Радиальные базисные функции (RBF) & 2 & 2 & 2 & 2 & 2 & 2 & 1 \\
        \hline
        Кригинг (Гауссовы процессы)       & 2 & 1 & 1 & 1 & 3 & 2 & 3 \\
        \hline
        Метод опорных векторов (SVM)      & 2 & 3 & 2 & 1 & 3 & 2 & 1 \\
        \hline
        Нейронные сети                    & 3 & 3 & 2 & 1 & 3 & 3 & 2 \\
        \hline
        Эволюционные алгоритмы            & 3 & 2 & 1 & 2 & 2 & 3 & 1 \\
        \midrule
    \end{tabular}
    \label{tab:method_evaluation}
\end{table}

Здесь числовые значения соответствуют оценкам из
морфологической матрицы (1 - низкая/плохая, 2 - средняя, 3 - высокая/хорошая).

Для наглядного представления результатов анализа на рисунке \ref{fig:morphological_analysis}
была приведена лепестковая диаграмма, отражающая оценки методов по каждому из
рассмотренных параметров.

\begin{figure}[ht]
    \centerfloat{
        \includegraphics{part4/morphological_analyse.pdf}
    }
    \caption{Лепестковые диаграммы каждого варианта}\label{fig:morphological_analysis}
\end{figure}

Для выбора оптимального метода необходимо учитывать важность
каждого параметра в контексте нашей конкретной задачи.
Учитывая специфику многокритериальной оптимизации алгоритмов
управления электропневматическими приводами с дискретными
распределителями, были присвоены следующие веса параметрам:

\begin{itemize}
    \item A: 0.25 -- высокая важность из-за нелинейности системы;
    \item B: 0.20 -- важно для работы с множеством параметров;
    \item C: 0.15 -- важно для итеративного процесса оптимизации;
    \item D: 0.05 -- менее важно для данной задачи;
    \item E: 0.20 -- важно для работы с новыми комбинациями параметров;
    \item F: 0.10 -- важно для работы с различными типами параметров;
    \item G: 0.05 -- менее важно для данной задачи.
\end{itemize}

Используя эти веса, была вычислена взвешенная сумма для каждого метода. Рассмотрим пример расчета взвешенной суммы для метода нейронных сетей:

\begin{equation}
    \begin{split}
        S_{НС} = & 0.25 \cdot 3 + 0.20 \cdot 3 + 0.15 \cdot 2 + 0.05 \cdot 1 + \\
        +        & 0.20 \cdot 3 + 0.10 \cdot 3 + 0.05 \cdot 2 =                \\
        =        & 0.75 + 0.60 + 0.30 + 0.05 + 0.60 + 0.30 + 0.10 =            \\
        =        & 2.60
    \end{split}
\end{equation}
где $S_{НС}$ -- взвешенная сумма для нейронных сетей, а числовые значения
соответствуют оценкам из таблицы \ref{tab:method_evaluation}.

Аналогичным образом были рассчитаны взвешенные суммы для остальных методов.
Результаты представлены в таблице \ref{tab:weighted_scores}.

\begin{table}[h]
    \centering
    \caption{Взвешенные оценки методов}
    \begin{tabular}{lc}
        \midrule
        Метод                             & Взвешенная сумма \\
        \midrule
        Полиномиальная регрессия          & 1.60             \\
        Радиальные базисные функции (RBF) & 1.95             \\
        Кригинг (Гауссовы процессы)       & 1.90             \\
        Метод опорных векторов (SVM)      & 2.25             \\
        Нейронные сети                    & 2.60             \\
        Эволюционные алгоритмы            & 2.15             \\
        \hline
    \end{tabular}
    \label{tab:weighted_scores}
\end{table}



Как видно из результатов, нейронные сети получили наивысшую взвешенную оценку
(2.60). Это объясняется тем, что они имеют высокие оценки по наиболее важным
критериям: способности к аппроксимации нелинейных зависимостей (вес 0.25),
масштабируемости (вес 0.20) и способности к обобщению (вес 0.20). Несмотря на
относительно низкую оценку по интерпретируемости результатов (1 с весом 0.05),
это не оказало значительного влияния на общий результат из-за низкого веса этого
критерия для нашей задачи.

На основе проведенного морфологического анализа с использованием метода
Цвикки, наиболее подходящим методом для построения суррогатной модели
в контексте многокритериальной оптимизации алгоритмов управления
электропневматическими приводами с дискретными распределителями
являются нейронные сети. Они получили наивысшую взвешенную оценку
благодаря своим сильным сторонам: высокой способности к аппроксимации
сложных нелинейных зависимостей, хорошей масштабируемости и работе с
высокоразмерными данными, высокой способности к обобщению и высокой
адаптивности к различным типам данных.

Несмотря на некоторые недостатки, такие как относительно низкая интерпретируемость
результатов и средняя вычислительная эффективность,
преимущества нейронных сетей в контексте данной задачи перевешивают
их недостатки. Для минимизации этих недостатков могут быть применены методы
регуляризации, техники визуализации и интерпретации нейронных сетей, а
также оптимизация архитектуры сети для повышения вычислительной эффективности.