\chapter{ОБЗОР МЕТОДОВ СУРРОГАТНОГО МОДЕЛИРОВАНИЯ}\label{chap:surrogate-modeling}

Суррогатные модели используются в качестве замены исходной модели для снижения трудоемкости
расчетов. В качестве таких моделей могут использоваться полиноминальная регрессия, радиальные
базисные функции и т.д. Ниже представлен обзор основных методов.

\paragraph{Полиномиальная регрессия}

Полиномиальная регрессия представляет собой метод аппроксимации данных
с использозванием полиномов различных степеней. Этот метод является расширением линейной
регрессии и позволяет моделировать зависисмости между входными и выходными переменными более
гибким образом, используя дополнительные нелинейные термины, такие как квадратичные, кубические члены
и~т.д., а также их взаимодействия \cite{fan2018local}.

Полиномиальная регрессия предполагает, что зависимость между предикторами и откликом
можеты быть описана полиноминальной функцией вида:

\begin{equation}
	\hat{y}(\mathbf{x}) = \beta_0 + \sum_{i=1}^{p} \beta_i x_i + \sum_{i=1}^{p} \sum_{j=1}^{p} \beta_{ij} x_i x_j + \ldots,
\end{equation}
где $\hat{y}(\mathbf{x})$ -- предсказанное значение отклика;
$\beta_0$ -- свободный член (интерцепт);
$\beta_i$ -- коэффициенты линейных членов $x_i$;
$\beta_{ij}$ -- коэффициенты взаимодействия между переменными $x_i$ и $x_j$;
$p$ -- количество перменных;
$x_i,~x_j$ -- входные переменные.

Для случая квадратичной регрессии полиноминальная функция принимает вид:

\begin{equation}
	\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2
	+ \beta_4 x_2^2 + \beta_5 x_1 x_2.
\end{equation}

Здесь учитываются коэффициенты линейных $\beta_1,~\beta_2$ и квадратичных $\beta_3,~\beta_4$ членов,
а также коэффициент взаимодействия $\beta_5$ \cite{heiberger2009polynomial}.

Для определения коэффициентов $\beta$ используется метод наименьших квадратов,
который минимизирует сумму квадратов отколнений между наблюдаемыми значениям $y_i$
и предсказанными $\hat{y}_i$. Задача оптимизации формулируется следующим образом:

\begin{equation}
	\min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 =
	\min_{\beta} \sum_{i=1}^{n} \left\{
	y_i - \left(\beta_0 + \sum_{j=1}^{p} \beta_j x_{ij}
	+ \sum_{j=1}^{p} \sum_{k=1}^{p} \beta_{jk} x_{ij} x_{ik}
	\right)
	\right\}^2,
\end{equation}
где $n$ -- количество наблюдений.

Для решения этой задачи вводится матричная форма:

\begin{equation}
	\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},
\end{equation}
где $\mathbf{y}$ -- вектор наблюдаемых значений откликов размерности $n \times 1$;
$\mathbf{X}$ -- матрица признаков размерности $n \times m$, где $m$ -- количество коэффициентов,
включая взаимодействия и нелинейные члены;
$\boldsymbol{\beta}$ -- вектор коэффициентов размерности $m \times 1$;
$\boldsymbol{\epsilon}$ -- вектор ошибок.

Оценка коэффицикнтов $\boldsymbol{\beta}$ производится с использованием псевдообратной матрицы:
\begin{equation}
	\boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.
\end{equation}
где $(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ -- псевдообратная матрица Мура-Пенроуза,
которая обеспечивает минимизацию ошибки на оценке коэффициентов \cite{meyer2009matrix}.

Преимущества полиномиальной регрессии для задачи оптимизации управления электропневматическими приводами:
\begin{enumerate}
	\item Простота реализации и интерпретации модели.
	\item Низкие вычислительные затраты на построение и использование модели.
	\item Возможность аналитического вычисления градиентов, что полезно для оптимизационных алгоритмов.
\end{enumerate}

Ограничения метода:
\begin{enumerate}
	\item Ограниченная способность моделировать сложные нелинейные зависимости, характерные для пневматических систем;
	\item Риск переобучения при использовании полиномов высоких степеней;
	\item Чувствительность к выбросам в экспериментальных данных.
\end{enumerate}

\paragraph{Радиальные базисные функции}

Метод радиальных базисных функций (RBF) представляет собой метод аппроксимации, основанный
на использовании базисных функций, значения которых зависят только от расстояния до центра.
Этот подход особенно эффективен для интерполяции многомерных разреженных данных и решения задач аппроксимации нелинейных зависимостей.

Основная идея метода RBF заключается в представлении интерполирующей функции в виде взвешенной суммы радиальных базисных функций:

\begin{equation}
\hat{y}(\mathbf{x}) = \sum_{i=1}^{n} w_i \phi(||\mathbf{x} - \mathbf{c}_i||),
\end{equation}
где $\hat{y}(\mathbf{x})$ -- аппроксимирующая функция;
$w_i$ -- весовые коэффициенты;
$\phi$ -- радиальная базисная функция;
$\mathbf{c}_i$ -- центры радиальных базисных функций;
$||\mathbf{x} - \mathbf{c}_i||$ -- расстояние между точкой $\mathbf{x}$ и центром $\mathbf{c}_i$.

Расстояние обычно вычисляется как евклидова норма:

\begin{equation}
||\mathbf{x} - \mathbf{c}_i|| = \sqrt{\sum_{j=1}^{d} (x_j - c_{ij})^2},
\end{equation}
где $d$ - размерность пространства входных переменных.

В качестве радиальных базисных функций $\phi(r)$ чаще всего используются следующие функции:

1. Гауссова функция:
\begin{equation}
\phi(r) = \exp\left(-\frac{r^2}{2\sigma^2}\right),
\end{equation}

2. Мультиквадратичная функция:
\begin{equation}
\phi(r) = \sqrt{r^2 + \sigma^2},
\end{equation}

3. Обратная мультиквадратичная функция:
\begin{equation}
\phi(r) = \frac{1}{\sqrt{r^2 + \sigma^2}},
\end{equation}

4. Линейная функция:
\begin{equation}
\phi(r) = r,
\end{equation}

5. Кубическая функция:
\begin{equation}
\phi(r) = r^3,
\end{equation}
где $\sigma$ -- параметр формы, определяющий ширину базисной функции.

Для определения весовых коэффициентов $w_i$ используется условие интерполяции,
согласно которому аппроксимирующая функция должна точно проходить через все экспериментальные точки:

\begin{equation}
\hat{y}(\mathbf{x}_j) = y_j, \quad j = 1, 2, \ldots, n,
\end{equation}
где $y_j$ -- значение функции в точке $\mathbf{x}_j$.

Подставляя это условие в выражение для аппроксимирующей функции, получаем систему линейных уравнений:

\begin{equation}
\sum_{i=1}^{n} w_i \phi(||\mathbf{x}_j - \mathbf{c}_i||) = y_j, \quad j = 1, 2, \ldots, n.
\end{equation}

В матричной форме эта система записывается как:

\begin{equation}
\mathbf{A} \mathbf{w} = \mathbf{y},
\end{equation}
где $\mathbf{A}$ -- матрица интерполяции размера $n \times n$ с элементами $A_{ji} = \phi(||\mathbf{x}_j - \mathbf{c}_i||)$;
$\mathbf{w}$ -- вектор весовых коэффициентов;
$\mathbf{y}$ -- вектор значений функции в экспериментальных точках.

Решение системы уравнений дает вектор весовых коэффициентов:

\begin{equation}
\mathbf{w} = \mathbf{A}^{-1} \mathbf{y}.
\end{equation}

Для улучшения гладкости аппроксимации и предотвращения переобучения часто применяется регуляризация:

\begin{equation}
\mathbf{w} = (\mathbf{A}^T\mathbf{A} + \lambda \mathbf{I})^{-1} \mathbf{A}^T \mathbf{y},
\end{equation}
где $\lambda$ -- параметр регуляризации, $\mathbf{I}$ -- единичная матрица.

В практических приложениях, особенно при моделировании сложных технических
систем, таких как пневмоприводы, важным аспектом является выбор параметра формы $\sigma$ и
расположения центров радиальных базисных функций. Для определения оптимальных значений этих
параметров используются различные методы, включая перекрестную проверку, алгоритмы оптимизации и эвристические подходы.

Метод RBF также может быть расширен для учета полиномиальных тенденций во входных данных путем добавления полиномиальных членов к базисным функциям:

\begin{equation}
\hat{y}(\mathbf{x}) = \sum_{i=1}^{n} w_i \phi(||\mathbf{x} - \mathbf{c}_i||) + \sum_{j=1}^{m} v_j p_j(\mathbf{x}),
\end{equation}
где $p_j(\mathbf{x})$ -- полиномиальные функции;
$v_j$ -- соответствующие коэффициенты;
$m$ -- количество полиномиальных членов.

Для обеспечения однозначности решения вводятся дополнительные условия ортогональности:

\begin{equation}
\sum_{i=1}^{n} w_i p_j(\mathbf{c}_i) = 0, \quad j = 1, 2, \ldots, m.
\end{equation}

Преимущества метода RBF:

\begin{enumerate}
	\item Способность эффективно аппроксимировать сложные нелинейные зависимости, характерные для пневматических систем.
	\item Хорошая обобщающая способность при правильном выборе параметров.
	\item Возможность точной интерполяции в экспериментальных точках.
	\item Универсальность применения для различных типов данных и размерностей пространства.
	\item Математическая элегантность и аналитическая простота базового метода.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
	\item Чувствительность к выбору параметров (количество и расположение центров, тип базисной функции, параметр формы).
	\item Потенциальные проблемы с обусловленностью матрицы интерполяции при большом количестве базисных функций.
	\item Сложность интерпретации модели по сравнению с полиномиальной регрессией.
	\item Вычислительная сложность для больших наборов данных.
	\item Необходимость предварительной нормализации входных данных для улучшения численной стабильности.
\end{enumerate}

\paragraph{Гауссовы процессы (Кригинг)}\label{sec:ch4/sec3/subsec1/subsubsec3}

Кригинг (Гауссовы процессы) является мощным методом интерполяции,
который позволяет строить суррогатные модели для сложных функций,
используя концепцию случайных процессов \cite{gramacy2020surrogates}. Он основан
на предположении, что процесс \( y(\mathbf{x}) \) может быть представлен в виде:

\begin{equation}
	y(\mathbf{x}) = \mu + Z(\mathbf{x}),
\end{equation}
где $\mu$ -- среднее значение; $Z(\mathbf{x})$ -- гауссовский процесс с нулевым средним и
ковариационной функцией \cite{marrel2024probabilistic}:

\begin{equation}
	\text{Cov}(Z(\mathbf{x}_i), Z(\mathbf{x}_j)) = k(\mathbf{x}_i, \mathbf{x}_j),
\end{equation}

Ковариационная функция часто задается как радиальная базисная функция:

\begin{equation}
	k(\mathbf{x}_i, \mathbf{x}_j) = \sigma^2 \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2l^2}\right),
\end{equation}.
где \( \sigma^2 \) — дисперсия;
$l$ — параметр длины \cite{figueroa2021gaussian}.

Предсказание значений в новых точках $\mathbf{x}_*$ осуществляется через условное распределение, учитывающее известные значения.
Среднее предсказание и его дисперсия определяются как:

\begin{equation}
	\hat{y}(\mathbf{x}_*) = \mu + \mathbf{k}_*^T \mathbf{K}^{-1} (\mathbf{y} - \mu \mathbf{1}_n),
\end{equation}

\begin{equation}
	\text{Var}(\hat{y}(\mathbf{x}_*)) = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*,
\end{equation}
где $\mathbf{k}_*$ — вектор ковариаций между новой точкой и обучающими точками;
$\mathbf{K}$ — ковариационная матрица \cite{zhou2020enhanced}.

Кригинг широко применяется в задачах многокритериальной оптимизации и позволяет
не только предсказывать значения, но и оценивать их неопределенность, что особенно
полезно при построении фронтов Парето и выборе компромиссных решений \cite{radaideh2020surrogate}.

Преимущества метода кригинга:

\begin{enumerate}
	\item Высокая точность интерполяции и экстраполяции.
	\item Возможность оценки неопределенности предсказаний.
	\item Гибкость в моделировании сложных нелинейных зависимостей.
	\item Эффективность при ограниченном количестве экспериментальных данных.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
	\item Вычислительная сложность при большом количестве экспериментальных точек.
	\item Чувствительность к выбору функции корреляции и ее параметров.
	\item Сложность интерпретации модели по сравнению с детерминированными методами.
\end{enumerate}

\paragraph{Метод опорных векторов.}

Метод опорных векторов (SVM) представляет собой один из наиболее эффективных методов классификации и
регрессии, основанный на поиске гиперплоскости, которая максимизирует зазор между классами.
Основная идея заключается в преобразовании исходных данных в более
высокое измерение с целью нахождения разделяющей гиперплоскости \cite{Jakkula2006}.

Рассмотрим обучающую выборку:
\begin{equation}
	\{(x_i, y_i)\}_{i=1}^N,
\end{equation}
где $x_i \in \mathbb{R}^n$ -- вектор признаков;
$y_i \in \{-1, 1\}$ -- метка класса.

Задача заключается в нахождении гиперплоскости,
которая разделяет два класса с максимальным зазором. Гиперплоскость определяется уравнением:

\begin{equation}
	f(x) = w^T x + b = 0,
\end{equation}
где $w \in \mathbb{R}^n$ -- вектор весов;
$b \in \mathbb{R}$ -- смещение.

Целью является минимизация следующей функции потерь с учетом ограничений:

\begin{equation}
	\min_{w, b} \frac{1}{2} \|w\|^2,
\end{equation}
при условиях:

\begin{equation}
	y_i (w^T x_i + b) \geq 1, \quad i = 1, \ldots, N.
\end{equation}

Для решения данной задачи применяется метод множителей Лагранжа,
что приводит к следующей двойственной задаче \cite{Patle2013}:

\begin{equation}
	\max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i^T x_j),
\end{equation}
при ограничениях:

\begin{equation}
	\sum_{i=1}^N \alpha_i y_i = 0, \quad \alpha_i \geq 0, \quad i = 1, \ldots, N.
\end{equation}
где $\alpha_i$ -- множители Лагранжа, которые определяют вклад каждого образца в решение задачи.

Для повышения мощности метода используется преобразование исходных данных
в пространство более высокой размерности с помощью ядровых
функций

\begin{equation}
	K(x_i, x_j) = \phi(x_i)^T \phi(x_j),
\end{equation}
где $\phi(\cdot)$ — отображение в новое
пространство признаков.

Распространенные ядра включают линейное,
полиномиальное и гауссово (радиальное базисное) ядро \cite{Deris2011}:

\begin{itemize}
	\item Линейное: $K(x_i, x_j) = x_i^T x_j$;
	\item Полиномиальное: $K(x_i, x_j) = (x_i^T x_j + 1)^d$;
	\item Гауссово: $K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$.
\end{itemize}

Для работы с шумными данными вводится параметр регуляризации $C$,
который контролирует баланс между шириной зазора и ошибками классификации.
Оптимизационная задача в этом случае принимает вид \cite{Boswell2002}:

\begin{equation}
	\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i,
\end{equation}

при условиях:

\begin{equation}
	y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i = 1, \ldots, N,
\end{equation}
где $\xi_i$ -- переменные, отвечающие за допущенные ошибки.

Преимущества метода опорных векторов:

\begin{enumerate}
	\item Высокая обобщающая способность, особенно при ограниченном наборе обучающих данных.
	\item Эффективность в задачах с большим количеством входных параметров.
	\item Способность моделировать сложные нелинейные зависимости.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
	\item Вычислительная сложность обучения модели для больших наборов данных.
	\item Чувствительность к выбору ядерной функции и настройке гиперпараметров.
	\item Сложность интерпретации модели по сравнению с более простыми методами, такими как полиномиальная регрессия.
\end{enumerate}

\paragraph{Нейронные сети}

Искусственные нейронные сети (ИНС) представляют собой вычислительные модели, основанные на
принципах организации и функционирования биологических нейронных сетей
\cite{goodfellow2016deep}. Базовый элемент нейросети -- искусственный нейрон -- может быть описан как:

\begin{equation}
    y = \sigma \left( \sum_{i=1}^{n} w_i x_i + b \right),
\end{equation}
где $x_i$ -- входные сигналы;
$w_i$ -- весовые коэффициенты;
$b$ -- смещение;
$\sigma(\cdot)$ -- функция активации;
$y$ -- выходной сигнал нейрона.

Многослойный перцептрон (MLP), состоящий из входного, скрытых
и выходного слоев, математически представляется как композиция функций:

\begin{equation}
    f(\mathbf{x}) = \sigma_L(W_L\sigma_{L-1}(W_{L-1}...\sigma_1(W_1 \mathbf{x} + b_1)... + b_{L-1}) + b_L),
\end{equation}
где $L$ -- количество слоев;
$W_l$ -- матрица весов для слоя $l$;
$b_l$ -- вектор смещений;
$\sigma_l$ -- функция активации.

Обучение нейронной сети осуществляется путем минимизации функции
потерь, характеризующей различие между предсказанными и фактическими значениями:

\begin{equation}
    \min_{W, b} \mathcal{L}(f(\mathbf{X}; W, b), \mathbf{Y}),
\end{equation}
где $\mathbf{X}$ -- матрица входных данных;
$\mathbf{Y}$ -- матрица целевых значений;
$\mathcal{L}$ -- функция потерь, например, среднеквадратичная ошибка для регрессионных задач:

\begin{equation}
    \mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - f(\mathbf{x}_i))^2.
\end{equation}

Для оптимизации параметров применяется алгоритм обратного распространения ошибки с различными модификациями
градиентного спуска \cite{Ghojogh2024Learning}. В задачах суррогатного моделирования динамических систем,
таких как пневмоприводы, используются специализированные архитектуры: сверточные нейронные сети (CNN) для обработки
пространственных данных \cite{Zhu2021Spatial}, рекуррентные сети (RNN, LSTM, GRU) для моделирования временных
последовательностей \cite{Zarzycki2021LSTM}, автоэнкодеры для снижения размерности \cite{Kaur2021Variational}. Значительное
влияние на качество модели оказывает выбор функций активации, таких как сигмоида, гиперболический тангенс, ReLU и их
модификации.

Преимущества нейронных сетей:

\begin{enumerate}
    \item Высокая способность к аппроксимации сложных нелинейных зависимостей, характерных для пневматических систем.
    \item Эффективное моделирование динамических процессов с помощью рекуррентных архитектур.
    \item Гибкость в выборе архитектуры и настройке гиперпараметров под конкретную задачу.
    \item Способность к инкрементному обучению и адаптации модели при получении новых экспериментальных данных.
\end{enumerate}

Ограничения метода:

\begin{enumerate}
    \item Необходимость значительного объема данных для обучения, особенно для глубоких архитектур.
    \item Сложность интерпретации внутренней структуры обученной модели (<<черный ящик>>).
    \item Риск переобучения, особенно при малых выборках и сложных архитектурах.
    \item Высокие вычислительные затраты на этапе обучения для сложных многослойных сетей.
\end{enumerate}

Для преодоления этих ограничений применяются техники регуляризации (L1, L2, Dropout), методы ранней остановки обучения,
ансамбли моделей и трансферное обучение \cite{Farhadi2022Combining}. В контексте многокритериальной оптимизации пневмоприводов
нейронные сети эффективно интегрируются с эволюционными алгоритмами, обеспечивая быструю оценку качества возможных решений
и ускоряя процесс поиска Парето-оптимальных конфигураций параметров управления.
