# МЕТОДОЛОГИЯ МНОГОКРИТЕРИАЛЬНОЙ ОПТИМИЗАЦИИ ПАРАМЕТРОВ ПНЕВМОПРИВОДА {#ch:ch4}

Задача многокритериальной оптимизации параметров пневмопривода
заключается в нахождении оптимального набора параметров системы
управления, обеспечивающего баланс между несколькими конфликтующими
критериями качества работы привода. Основными критериями в данном случае
выступают точность позиционирования выходного звена пневмопривода и
частота переключений дискретных пневмораспределителей, которые
определяют долговечность и энергопотребление системы. Оптимизация
параметров управления направлена на минимизацию ошибок позиционирования
при минимально возможном числе переключений, что позволяет продлить срок
службы оборудования и снизить износ компонентов.

## Постановка задачи многокритериальной оптимизации параметров пневмопривода {#ch:ch4/sec1}

Для решения задачи применяются методы построения фронта Парето,
позволяющие выделить множество неулучшаемых решений, представляющих
компромисс между критериями. Оптимизационная задача формулируется как
поиск таких значений параметров управления, при которых улучшается один
из критериев, не ухудшая при этом другие. Это достигается путем
численного моделирования системы с использованием суррогатных моделей,
которые сокращают вычислительные затраты и позволяют быстро оценивать
показатели качества при различных сочетаниях параметров. Результаты
оптимизации используются для выбора наилучшей стратегии управления
пневмоприводом в зависимости от заданных условий эксплуатации и
требований к точности и ресурсам системы.

### Концепция оптимальности по Парето {#ch:ch4/sec1/subsec1}

Концепция оптимальности по Парето, предложенная итальянским экономистом
Вильфредо Парето [-@pareto1896cours] в конце XIX века, является
фундаментальным понятием в теории многокритериальной оптимизации. Данная
концепция предоставляет математический аппарат для анализа и принятия
решений [-@miettinen1999nonlinear] в ситуациях, где необходимо
одновременно оптимизировать несколько, зачастую противоречивых,
критериев.

Рассмотрим задачу многокритериальной оптимизации с $k$ целевыми
функциями [-@deb2001multi]:

$$\min_{x \in \Omega} F(x) = (\min f_1(x), \min f_2(x), \ldots, \min f_k(x)),$$
где $x \in \Omega \subset \mathbb{R}^n$ -- вектор решений, принадлежащий
допустимому множеству $\Omega$; $F: \Omega \rightarrow \mathbb{R}^k$ --
векторная целевая функция.

Решение $x^* \in \Omega$ называется оптимальным по Парето (или
Парето-оптимальным), если не существует другого решения $x \in \Omega$,
такого что:

$$\begin{cases}
        \forall i \in \{1, \ldots, k\}: f_i(x) \leq f_i(x^*), \\
        \exists j \in \{1, \ldots, k\}: f_j(x) < f_j(x^*).
    \end{cases}$$

Иными словами, решение является Парето-оптимальным, если невозможно
улучшить значение любого критерия без ухудшения значения хотя бы одного
другого критерия.

Концепция оптимальности по Парето тесно связана с понятием
доминирования. Говорят, что решение $x_1$ доминирует решение $x_2$
(обозначается как $x_1 \prec x_2$), если выполняются следующие условия:

$$\begin{cases}
        \forall i \in \{1, \ldots, k\}: f_i(x_1) \leq f_i(x_2), \\
        \exists j \in \{1, \ldots, k\}: f_j(x_1) < f_j(x_2).
    \end{cases}$$

Множество всех Парето-оптимальных решений образует множество
недоминируемых решений, которое также называется множеством Парето или
Парето-множеством.

Образ множества Парето в пространстве критериев называется фронтом
Парето. Математически фронт Парето можно определить как:

$$PF = \{F(x) | x \in PS\},$$ где $PS$ -- множество Парето в
пространстве решений.

Фронт Парето представляет собой геометрическое место точек
[-@coello2007evolutionary] в пространстве критериев, соответствующих
недоминируемым решениям. Он наглядно демонстрирует компромиссы между
различными целевыми функциями и играет ключевую роль в процессе принятия
решений.

На рисунке [1.1](#fig:pareto_front_example){reference-type="ref"
reference="fig:pareto_front_example"} приведен пример фронта Парето для
двух критериев.

<figure id="fig:pareto_front_example">

<figcaption>Пример фронта Парето для двух критериев.</figcaption>
</figure>

\[ht\] На рисунке видно, что фронт Парето представляет собой кривую,
состоящую из недоминируемых решений (точек), для которых невозможно
улучшить один критерий без ухудшения другого.

Основные свойства оптимальности по Парето:

1.  Несравнимость: Парето-оптимальные решения несравнимы между собой в
    смысле доминирования.

2.  Иерархическая структура: Концепция Парето-оптимальности может быть
    расширена на случай иерархической оптимизации, где критерии имеют
    различные приоритеты.

3.  Инвариантность: Парето-оптимальные решения инвариантны относительно
    монотонных преобразований целевых функций.

4.  Выпуклость: Если все целевые функции выпуклы и допустимое множество
    выпукло, то множество Парето также выпукло.

Для нахождения множества Парето-оптимальных решений в задачах
многокритериальной оптимизации используются специальные эволюционные
алгоритмы. Эти методы работают с популяциями решений и постепенно
улучшают их, используя механизмы, подобные естественному отбору. Ниже
описаны два из наиболее известных алгоритмов для поиска
Парето-оптимальных решений.

Сущществует множество методов для поиска Парето-оптимальных решений,
однако наиболее распространенными являются эволюционные алгоритмы, такие
как NSGA-II и SPEA2.

NSGA-II --- это один из самых популярных эволюционных алгоритмов для
многокритериальной оптимизации. Его ключевые особенности:

1.  Сортировка по доминированию: На каждом шаге алгоритм делит популяцию
    на несколько уровней, исходя из степени доминирования решений. Те
    решения, которые не доминируются другими, попадают в первый уровень,
    остальные сортируются в соответствии с их степенью доминирования.

2.  Crowding Distance: NSGA-II использует метрику \"crowding distance\"
    для оценки плотности решений в области фронта Парето. Это помогает
    поддерживать разнообразие решений, предотвращая их слияние в одном
    месте.

3.  Операторы отбора, скрещивания и мутаций: Алгоритм применяет
    стандартные операторы генетического алгоритма для эволюции популяции
    --- отбор лучших решений, скрещивание и мутацию для создания нового
    поколения.

NSGA-II обеспечивает эффективное нахождение множества Парето-оптимальных
решений, а также хорошо сохраняет разнообразие решений вдоль фронта
Парето [-@deb2001multi].

SPEA2 --- это улучшенная версия алгоритма SPEA, разработанная для
повышения эффективности поиска недоминируемых решений. Основные
улучшения SPEA2 включают:

1.  Архивирование решений: SPEA2 сохраняет архив недоминируемых решений
    на каждом шаге, что помогает гарантировать, что фронт Парето не
    будет потерян в процессе эволюции.

2.  Оценка решений: Каждый элемент популяции получает оценку на основе
    того, сколько решений он доминирует и насколько сильно доминируется
    сам. Эта оценка используется для выбора кандидатов для следующего
    поколения.

3.  Учет плотности решений: Подобно NSGA-II, SPEA2 учитывает плотность
    решений вблизи каждого кандидата, что помогает поддерживать
    разнообразие и улучшать распределение решений вдоль фронта Парето.

SPEA2 продемонстрировал высокую производительность на сложных задачах
многокритериальной оптимизации и может эффективно находить множество
недоминируемых решений [@zitzler2001spea2].

В задаче оптимизации параметров управления пневматическим приводом
концепция оптимальности по Парето позволяет учесть множественные,
зачастую противоречивые, критерии качества управления. Например,
минимизация времени переходного процесса и минимизация колличества
переключений распределителей могут находиться в конфликте друг с другом.
Построение фронта Парето в этом случае позволяет выявить множество
оптимальных компромиссных решений и предоставить лицу, принимающему
решения, полную картину возможных вариантов.

## Методы построения сурогантных моделей {#sec:ch4/sec2}

Суррогатное моделирование является достаточно эффектиынвм методом в
задачах многокритериальной оптимизации, особенно при построении
Парето-фронта. Как правило, оно используется для аппроксимации сложных и
вычислительно затратных функций или систем, что позволяет значительно
снизить временные и вычислительные затраты на решение задач, требующих
многократной оценки целевых функций. Данный подход особенно полезен в
тех случаях, когда каждая отдельная оценка целевой функции является
дорогостоящей с точки зрения времени или ресурсов, как это часто
встречается при численном моделировании физических процессов или
компьютерных симуляциях сложных инженерных систем.

Для синжения Для снижения вычислительных затрат используется суррогатное
моделирование, при котором сложные функции аппроксимируются с помощью
более простых моделей, называемых суррогатами, которые можно быстро и
эффективно вычислять. Это позволяет оптимизировать процесс поиска
Парето-фронта и находить решения с меньшими затратами ресурсов.

### Обзор методов суррогатного моделирования {#sec:ch4/sec2/subsec1}

##### Полиномиальная регрессия {#sec:ch4/sec2/subsec1/subsubsec1}

Полиномиальная регрессия представляет собой метод аппроксимации данных с
использозванием полиномов различных степеней. Этот метод является
расширением линейной регрессии и позволяет моделировать зависисмости
между входными и выходными переменными более гибким образом, используя
дополнительные нелинейные термины, такие как квадратичные, кубические
члены и т.д., а также их взаимодействия [@fan2018local].

Полиномиальная регрессия предполагает, что зависимость между
предикторами и откликом можеты быть описана полиноминальной функцией
вида:

$$\hat{y}(\mathbf{x}) = \beta_0 + \sum_{i=1}^{p} \beta_i x_i + \sum_{i=1}^{p} \sum_{j=1}^{p} \beta_{ij} x_i x_j + \ldots,$$
где $\hat{y}(\mathbf{x})$ -- предсказанное значение отклика; $\beta_0$
-- свободный член (интерцепт); $\beta_i$ -- коэффициенты линейных членов
$x_i$; $\beta_{ij}$ -- коэффициенты взаимодействия между переменными
$x_i$ и $x_j$; $p$ -- количество перменных; $x_i,~x_j$ -- входные
переменные.

Для случая квадратичной регрессии полиноминальная функция принимает вид:

$$\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1^2
    + \beta_4 x_2^2 + \beta_5 x_1 x_2.$$

Здесь учитываются коэффициенты линейных $\beta_1,~\beta_2$ и
квадратичных $\beta_3,~\beta_4$ членов, а также коэффициент
взаимодействия $\beta_5$ [@heiberger2009polynomial].

Для определения коэффициентов $\beta$ используется метод наименьших
квадратов, который минимизирует сумму квадратов отколнений между
наблюдаемыми значениям $y_i$ и предсказанными $\hat{y}_i$. Задача
оптимизации формулируется следующим образом:

$$\min_{\beta} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 =
    \min_{\beta} \sum_{i=1}^{n} \left\{
    y_i - \left(\beta_0 + \sum_{j=1}^{p} \beta_j x_{ij}
    + \sum_{j=1}^{p} \sum_{k=1}^{p} \beta_{jk} x_{ij} x_{ik}
    \right)
    \right\}^2,$$ где $n$ -- количество наблюдений.

Для решения этой задачи вводится матричная форма:

$$\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon},$$
где $\mathbf{y}$ -- вектор наблюдаемых значений откликов размерности
$n \times 1$; $\mathbf{X}$ -- матрица признаков размерности
$n \times m$, где $m$ -- количество коэффициентов, включая
взаимодействия и нелинейные члены; $\boldsymbol{\beta}$ -- вектор
коэффициентов размерности $m \times 1$; $\boldsymbol{\epsilon}$ --
вектор ошибок.

Оценка коэффицикнтов $\boldsymbol{\beta}$ производится с использованием
псевдообратной матрицы:
$$\boldsymbol{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y}.$$
где $(\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T$ -- псевдообратная
матрица Мура-Пенроуза, которая обеспечивает минимизацию ошибки на оценке
коэффициентов [@meyer2009matrix].

Преимущества полиномиальной регрессии для задачи оптимизации управления
электропневматическими приводами:

1.  Простота реализации и интерпретации модели.

2.  Низкие вычислительные затраты на построение и использование модели.

3.  Возможность аналитического вычисления градиентов, что полезно для
    оптимизационных алгоритмов.

Ограничения метода:

1.  Ограниченная способность моделировать сложные нелинейные
    зависимости, характерные для пневматических систем;

2.  Риск переобучения при использовании полиномов высоких степеней;

3.  Чувствительность к выбросам в экспериментальных данных.

##### Радиальные базисные функции {#sec:ch4/sec2/subsec1/subsubsec2}

Преимущества метода RBF для моделирования электропневматических
приводов:

1.  Способность эффективно аппроксимировать сложные нелинейные
    зависимости.

2.  Хорошая обобщающая способность при правильном выборе параметров.

3.  Возможность точной интерполяции в экспериментальных точках.

Ограничения метода:

1.  Чувствительность к выбору параметров (количество и расположение
    центров, тип базисной функции, параметр формы).

2.  Потенциальные проблемы с обусловленностью матрицы интерполяции при
    большом количестве базисных функций.

3.  Сложность интерпретации модели по сравнению с полиномиальной
    регрессией.

##### Гауссовы процессы (Кригинг) {#sec:ch4/sec2/subsec1/subsubsec3}

Кригинг (Гауссовы процессы) является мощным методом интерполяции,
который позволяет строить суррогатные модели для сложных функций,
используя концепцию случайных процессов [@gramacy2020surrogates]. Он
основан на предположении, что процесс $y(\mathbf{x})$ может быть
представлен в виде:

$$y(\mathbf{x}) = \mu + Z(\mathbf{x}),$$ где $\mu$ -- среднее значение;
$Z(\mathbf{x})$ -- гауссовский процесс с нулевым средним и
ковариационной функцией [@marrel2024probabilistic]:

$$\text{Cov}(Z(\mathbf{x}_i), Z(\mathbf{x}_j)) = k(\mathbf{x}_i, \mathbf{x}_j),$$

Ковариационная функция часто задается как радиальная базисная функция:

$$k(\mathbf{x}_i, \mathbf{x}_j) = \sigma^2 \exp\left(-\frac{\|\mathbf{x}_i - \mathbf{x}_j\|^2}{2l^2}\right),$$.
где $\sigma^2$ --- дисперсия; $l$ --- параметр длины
[@figueroa2021gaussian].

Предсказание значений в новых точках $\mathbf{x}_*$ осуществляется через
условное распределение, учитывающее известные значения. Среднее
предсказание и его дисперсия определяются как:

$$\hat{y}(\mathbf{x}_*) = \mu + \mathbf{k}_*^T \mathbf{K}^{-1} (\mathbf{y} - \mu \mathbf{1}_n),$$

$$\text{Var}(\hat{y}(\mathbf{x}_*)) = k(\mathbf{x}_*, \mathbf{x}_*) - \mathbf{k}_*^T \mathbf{K}^{-1} \mathbf{k}_*,$$
где $\mathbf{k}_*$ --- вектор ковариаций между новой точкой и обучающими
точками; $\mathbf{K}$ --- ковариационная матрица [@zhou2020enhanced].

Кригинг широко применяется в задачах многокритериальной оптимизации и
позволяет не только предсказывать значения, но и оценивать их
неопределенность, что особенно полезно при построении фронтов Парето и
выборе компромиссных решений [@radaideh2020surrogate].

Преимущества метода кригинга для моделирования электропневматических
приводов:

1.  Высокая точность интерполяции и экстраполяции;

2.  Возможность оценки неопределенности предсказаний;

3.  Гибкость в моделировании сложных нелинейных зависимостей;

4.  Эффективность при ограниченном количестве экспериментальных данных.

Ограничения метода:

1.  Вычислительная сложность при большом количестве экспериментальных
    точек;

2.  Чувствительность к выбору функции корреляции и ее параметров;

3.  Сложность интерпретации модели по сравнению с детерминированными
    методами.

##### Метод опорных векторов.

Метод опорных векторов (SVM) представляет собой один из наиболее
эффективных методов классификации и регрессии, основанный на поиске
гиперплоскости, которая максимизирует зазор между классами. Основная
идея заключается в преобразовании исходных данных в более высокое
измерение с целью нахождения разделяющей гиперплоскости [@Jakkula2006].

Рассмотрим обучающую выборку: $$\{(x_i, y_i)\}_{i=1}^N,$$ где
$x_i \in \mathbb{R}^n$ -- вектор признаков; $y_i \in \{-1, 1\}$ -- метка
класса.

Задача заключается в нахождении гиперплоскости, которая разделяет два
класса с максимальным зазором. Гиперплоскость определяется уравнением:

$$f(x) = w^T x + b = 0,$$ где $w \in \mathbb{R}^n$ -- вектор весов;
$b \in \mathbb{R}$ -- смещение.

Целью является минимизация следующей функции потерь с учетом
ограничений:

$$\min_{w, b} \frac{1}{2} \|w\|^2,$$ при условиях:

$$y_i (w^T x_i + b) \geq 1, \quad i = 1, \ldots, N.$$

Для решения данной задачи применяется метод множителей Лагранжа, что
приводит к следующей двойственной задаче [@Patle2013]:

$$\max_{\alpha} \sum_{i=1}^N \alpha_i - \frac{1}{2} \sum_{i=1}^N \sum_{j=1}^N \alpha_i \alpha_j y_i y_j (x_i^T x_j),$$
при ограничениях:

$$\sum_{i=1}^N \alpha_i y_i = 0, \quad \alpha_i \geq 0, \quad i = 1, \ldots, N.$$
где $\alpha_i$ -- множители Лагранжа, которые определяют вклад каждого
образца в решение задачи.

Для повышения мощности метода используется преобразование исходных
данных в пространство более высокой размерности с помощью ядровых
функций

$$K(x_i, x_j) = \phi(x_i)^T \phi(x_j),$$ где $\phi(\cdot)$ ---
отображение в новое пространство признаков.

Распространенные ядра включают линейное, полиномиальное и гауссово
(радиальное базисное) ядро [@Deris2011]:

-   Линейное: $K(x_i, x_j) = x_i^T x_j$;

-   Полиномиальное: $K(x_i, x_j) = (x_i^T x_j + 1)^d$;

-   Гауссово:
    $K(x_i, x_j) = \exp\left(-\frac{\|x_i - x_j\|^2}{2\sigma^2}\right)$.

Для работы с шумными данными вводится параметр регуляризации $C$,
который контролирует баланс между шириной зазора и ошибками
классификации. Оптимизационная задача в этом случае принимает вид
[@Boswell2002]:

$$\min_{w, b, \xi} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^N \xi_i,$$

при условиях:

$$y_i (w^T x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0, \quad i = 1, \ldots, N,$$
где $\xi_i$ -- переменные, отвечающие за допущенные ошибки.

Преимущества метода опорных векторов для построения суррогатной модели
электропневматического привода:

1.  Высокая обобщающая способность, особенно при ограниченном наборе
    обучающих данных;

2.  Эффективность в задачах с большим количеством входных параметров;

3.  Способность моделировать сложные нелинейные зависимости.

Ограничения метода:

1.  \- Вычислительная сложность обучения модели для больших наборов
    данных;

2.  Чувствительность к выбору ядерной функции и настройке
    гиперпараметров;

3.  Сложность интерпретации модели по сравнению с более простыми
    методами, такими как полиномиальная регрессия.

##### Нейронные сети {#sec:ch4/sec2/subsec1/subsubsec5}

##### Эволюционные алгоритмы и метаэвристики {#sec:ch4/sec2/subsec1/subsubsec6}

Эволюционные алгоритмы и метаэвристики представляют собой группу методов
оптимизации, вдохновленных природными процессами. Они основанны на
принципа биологической эвлюции и предлагают подходы, такие как
генетические алгоритмы, алгоритмы роя частиц, иммунные алгоритмы и др.,
которые позволяют эффективно искать решения в пространстве возможных
параметров [@zhang2015comparision].

Основная идея эволюционных алгоритмов заключается в имитации процесса
естественного отбора, где популяция решений обновляется с каждым шагом
алгоритма, и только наилучшие решения сохраняются для следующего
поколения. Алгоритм начинается с инициализации популяции случайных
решений, где каждое решение представляет собой набор параметров
управления. Далее, для каждой особи в популяции оценивается функция
приспособленности, которая определяет, насколько хорошо решение
выполняет поставленную задачу. На основе функции приспособленности
отбираются лучшие решения, которые подвергаются мутации и скрещиванию,
чтобы создать новые решения, и процесс повторяется до тех пор, пока не
будет достигнута заданная точность или не исчерпаны вычислительные
ресурсы.

Математически, решение $\mathbf{x}$ оптимизируется с помощью
эволюционного алгоритма, следуюя процедуре:

$$x^{t+1} = \text{ОТБОР} \left\{
    \text{МУТАЦИЯ} \left\{
    \text{СКРЕЩИВАНИЕ} \left\{
    x^t
    \right\}
    \right\}
    \right\},$$ где $x^t$ -- текущее решение; $x^{t+1}$ -- новое
решение; $\text{ОТБОР}$ -- процедура отбора лучших решений или решений с
высокой приспособленностью; $\text{МУТАЦИЯ}$ -- процедура случайного или
направленного изменения решения; $\text{СКРЕЩИВАНИЕ}$ -- процедура
комбинирования решений для создания новых вариантов.

Основные элементы эволюционного алгоритма можно описать следующим
образом:

1.  Инициализация популяции: создание случайной популяции решений
    $x_i,~i = 1, \ldots, N$;

2.  Оценка приспособленности: для каждого решения рассчитвыается функция
    приспособленности $f(x_i)$, определяющая качество решения;

3.  Отбор: выбор лучших решений для создания нового решения;

4.  Скрещивание и мутация: создаются новые решения путем комбинирования
    и изменения существующих;

5.  Замена: новые решения заменяют старые в популяции и процесс
    повторяется до достижения критерия останова.

Основные трудности применения эволюционных алгоритмов заключается в
определении параметров алгоритма, таких как размер популяции,
вероятность мутации и скрещивания, а также в обеспечении достаточной
вычислительной мощности для обработки большого числа итераций.

Метаэврестические подходы, такие как алгоритмы роя частиц и методы
имитации отжига, дополняют эволлюционные алгоритмы, предоставляя
дополнительные инструменты для исследования пространства решений. Эти
методы доказали свою эффективность в решении задач многокритериальной
оптимизации, где необходимо сбалансировать несколько противоречивых
показателей качества.

Например, алгоритмы роя частиц (PSO) имитируют поведение стаци, где
каждый агент (частица) премещается в пространтсве решений с учетом своей
собственной истории и информации, полученной от других агентов. Частица
$i$ обновляет свою скорость и положение следующим образом:

$$\begin{aligned}
        v_i^{t+1} & = \omega \v_i^t + c_1 r_1 (p_i^t - x_i^t) + c_2 r_2 (g^t - x_i^t), \\
        x_i^{t+1} & = x_i^t + v_i^{t+1},
    \end{aligned}$$ где $\omega$ -- коэффициент инерции; $c_1, c_2$ --
коэффициенты обучения; $r_1, r_2$ -- случайные числа; $p_i$ -- лучшее
положение частицы; $g$ -- лучшее положение частицы.

Данные методы позволяют эффективно находить компромисные решения в
условиях многокритериальной оптимизации, что делает их особенно
полезными при управлении пневмоприводами и другими сложными системами,
где требуется балансировать между различными критериями, такими как
точность и частота переключеий.

Преимущества эволюционных алгоритмов и метаэвристик для моделирования
электропневматических приводов:

1.  Способность находить компромисные решения в условиях
    многокритериальной оптимизации;

2.  Эффективность в поиске глобальных оптимумов в пространстве
    параметров;

3.  Легкость в адаптации для многокритериальной оптимизации, что
    позволяет учитывать различные критерии качества.

Ограничения методов:

1.  Высокая вычислительная сложность при большом количестве параметров;

2.  Чувствительность к выбору параметров алгоритма;

3.  Нет гарантии нахождения глобального оптимума.

### Выбор оптимального метода построения суррогатных моделей {#sec:ch4/sec2/subsec2}

В рамках исследования методов построения суррогатных моделей для
многокритериальной оптимизации алгоритмов управления
электропневматическими приводами с дискретными распределителями был
применен метод морфологического анализа Фрица Цвикки. Данный метод
позволяет систематически рассмотреть все возможные решения проблемы
путем анализа всех комбинаций параметров, что особенно важно при выборе
оптимального подхода в сложных многопараметрических задачах.

Метод Цвикки включает в себя несколько этапов. На первом этапе
формулируется проблема и определяются ключевые параметры,
характеризующие возможные решения. В нашем случае, ключевыми параметрами
для оценки методов построения суррогатных моделей были выбраны:

A. Способность к аппроксимации нелинейных зависимостей B.
Масштабируемость C. Вычислительная эффективность D. Интерпретируемость
результатов E. Способность к обобщению F. Адаптивность к типам данных G.
Оценка неопределенности

Для каждого параметра были определены возможные значения: низкое,
среднее и высокое (или эквивалентные им). Это позволяет создать
морфологическую матрицу, которая представляет собой многомерное
пространство возможных решений.

::: {#tab:morphological_matrix}
  Параметр                             Значение 1   Значение 2   Значение 3
  ----------------------------------- ------------ ------------ ------------
  A. Способность к аппроксимации         Низкая      Средняя      Высокая
  нелинейных зависимостей                                       
  B. Масштабируемость                    Плохая      Средняя      Хорошая
  C. Вычислительная эффективность        Низкая      Средняя      Высокая
  D. Интерпретируемость результатов      Низкая      Средняя      Высокая
  E. Способность к обобщению             Низкая      Средняя      Высокая
  F. Адаптивность к типам данных         Низкая      Средняя      Высокая
  G. Оценка неопределенности              Нет       Частичная      Полная

  : Морфологическая матрица методов построения суррогатных моделей
:::

[]{#tab:morphological_matrix label="tab:morphological_matrix"}

На следующем этапе анализа каждый рассматриваемый метод построения
суррогатных моделей был оценен по каждому параметру. Оценка проводилась
на основе теоретических свойств методов и опыта их применения в схожих
задачах. Результаты оценки представлены в следующей таблице:

::: {#tab:method_evaluation}
  Метод                                A   B   C   D   E   F   G
  ----------------------------------- --- --- --- --- --- --- ---
  Полиномиальная регрессия             1   1   3   3   1   1   1
  Радиальные базисные функции (RBF)    2   2   2   2   2   2   1
  Кригинг (Гауссовы процессы)          2   1   1   1   3   2   3
  Метод опорных векторов (SVM)         2   3   2   1   3   2   1
  Нейронные сети                       3   3   2   1   3   3   2
  Эволюционные алгоритмы               3   2   1   2   2   3   1

  : Оценка методов по параметрам
:::

[]{#tab:method_evaluation label="tab:method_evaluation"}

Здесь числовые значения соответствуют оценкам из морфологической матрицы
(1 - низкая/плохая, 2 - средняя, 3 - высокая/хорошая).

Для наглядного представления результатов анализа на рисунке
[1.2](#fig:morphological_analysis){reference-type="ref"
reference="fig:morphological_analysis"} была приведена лепестковая
диаграмма, отражающая оценки методов по каждому из рассмотренных
параметров.

<figure id="fig:morphological_analysis">

<figcaption>Лепестковые диаграммы каждого варианта</figcaption>
</figure>

Для выбора оптимального метода необходимо учитывать важность каждого
параметра в контексте нашей конкретной задачи. Учитывая специфику
многокритериальной оптимизации алгоритмов управления
электропневматическими приводами с дискретными распределителями, были
присвоены следующие веса параметрам:

-   A: 0.25 -- высокая важность из-за нелинейности системы;

-   B: 0.20 -- важно для работы с множеством параметров;

-   C: 0.15 -- важно для итеративного процесса оптимизации;

-   D: 0.05 -- менее важно для данной задачи;

-   E: 0.20 -- важно для работы с новыми комбинациями параметров;

-   F: 0.10 -- важно для работы с различными типами параметров;

-   G: 0.05 -- менее важно для данной задачи.

Используя эти веса, была вычислена взвешенная сумма для каждого метода.
Рассмотрим пример расчета взвешенной суммы для метода нейронных сетей:

$$\begin{split}
        S_{НС} = & 0.25 \cdot 3 + 0.20 \cdot 3 + 0.15 \cdot 2 + 0.05 \cdot 1 + \\
        +        & 0.20 \cdot 3 + 0.10 \cdot 3 + 0.05 \cdot 2 =                \\
        =        & 0.75 + 0.60 + 0.30 + 0.05 + 0.60 + 0.30 + 0.10 =            \\
        =        & 2.60
    \end{split}$$ где $S_{НС}$ -- взвешенная сумма для нейронных сетей,
а числовые значения соответствуют оценкам из таблицы
[1.2](#tab:method_evaluation){reference-type="ref"
reference="tab:method_evaluation"}.

Аналогичным образом были рассчитаны взвешенные суммы для остальных
методов. Результаты представлены в таблице
[1.3](#tab:weighted_scores){reference-type="ref"
reference="tab:weighted_scores"}.

::: {#tab:weighted_scores}
  Метод                                Взвешенная сумма
  ----------------------------------- ------------------
  Полиномиальная регрессия                   1.60
  Радиальные базисные функции (RBF)          1.95
  Кригинг (Гауссовы процессы)                1.90
  Метод опорных векторов (SVM)               2.25
  Нейронные сети                             2.60
  Эволюционные алгоритмы                     2.15

  : Взвешенные оценки методов
:::

[]{#tab:weighted_scores label="tab:weighted_scores"}

Как видно из результатов, нейронные сети получили наивысшую взвешенную
оценку (2.60). Это объясняется тем, что они имеют высокие оценки по
наиболее важным критериям: способности к аппроксимации нелинейных
зависимостей (вес 0.25), масштабируемости (вес 0.20) и способности к
обобщению (вес 0.20). Несмотря на относительно низкую оценку по
интерпретируемости результатов (1 с весом 0.05), это не оказало
значительного влияния на общий результат из-за низкого веса этого
критерия для нашей задачи.

На основе проведенного морфологического анализа с использованием метода
Цвикки, наиболее подходящим методом для построения суррогатной модели в
контексте многокритериальной оптимизации алгоритмов управления
электропневматическими приводами с дискретными распределителями являются
нейронные сети. Они получили наивысшую взвешенную оценку благодаря своим
сильным сторонам: высокой способности к аппроксимации сложных нелинейных
зависимостей, хорошей масштабируемости и работе с высокоразмерными
данными, высокой способности к обобщению и высокой адаптивности к
различным типам данных.

Несмотря на некоторые недостатки, такие как относительно низкая
интерпретируемость результатов и средняя вычислительная эффективность,
преимущества нейронных сетей в контексте данной задачи перевешивают их
недостатки. Для минимизации этих недостатков могут быть применены методы
регуляризации, техники визуализации и интерпретации нейронных сетей, а
также оптимизация архитектуры сети для повышения вычислительной
эффективности.

## Разработка нейросетевой суррогатной модели {#sec:ch4/sec3}

В рамках разработки нейросетевой суррогатной модели для
многокритериальной оптимизации алгоритмов управления
электропневматическими приводами была предложена архитектура, основанная
на концепции остаточных блоков. Данный подход позволяет эффективно
обучать глубокие сети, преодолевая проблему затухающих градиентов.
Структура модели включает входной слой, принимающий вектор параметров
настройки алгоритма управления, последовательность остаточных блоков и
выходной линейный слой, предсказывающий значения критериев качества
управления.

Для повышения обобщающей способности модели и снижения риска
переобучения применяется техника регуляризации. Процесс разработки
суррогатной модели включал в себя оптимизацию гиперпараметров, таких как
количество остаточных блоков, размерности скрытых слоев и скорость
обучения, с использованием байесовской оптимизации. Данный подход
позволил создать эффективную суррогатную модель, способную точно
аппроксимировать сложные нелинейные зависимости между параметрами
алгоритма управления и критериями качества электропневматического
привода.

### Архитектура модели {#sec:ch4/sec3/subsec1}

В целях повышения эффективности и точности суррогатного моделирования
была разработана нейронная сеть, основанная на архитектуре с
использованием остаточных блоков (Residual Blocks). Данная архитектура
была выбрана для обеспечения устойчивости обучения и возможности
построения глубоких моделей, способных захватывать сложные нелинейные
зависимости между входными параметрами и выходными метриками системы.

##### Структура нейронной сети {#sec:ch4/sec3/subsec1/subsubsec1}

Разработанная нейронная сеть представляет собой глубокую многослойную
перцептронную модель (Multilayer Perceptron, MLP)
[-@goodfellow2016deep], интегрированную с остаточными блоками для
улучшения процесса обучения и повышения обобщающей способности модели.
Архитектура сети состоит из следующих компонентов:

1.  Входной слой: входной слой принимает вектор признаков
    $\mathbf{x} \in \mathbb{R}^n$, представляющий праметры инициализации
    системы. В данном случае -- это парметры, которые изменяются в
    процессе оптимизации алгоритма. Колличестов нейронов во входном слое
    соответствует размерности входного вектора $n$;

2.  Последовательность остаточных блоков: основная часть сети состоит из
    серии остаточных блоков, каждый из которых включает в себя два
    линейных слоя с последующей нормализацией батча [-@ioffe2015batch],
    функцией активации ReLU и Dropout для регуляризации. Остаточные
    блоки реализуются для обеспечения прямого прохождения градиентов и
    предотвращения проблем, связанных с обучением глубоких сетей, таких
    как исчезающие градиенты [-@he2016deep].

3.  Финальный линейный слой: После последовательности остаточных блоков
    добавляется финальный линейный слой, количество нейронов которого
    соответствует размерности выходных метрик $m$. Этот слой отвечает за
    преобразование скрытых представлений в предсказания модели.

Рассмотрим каждый элемент архитектуры подробнее. Фундаментальным
структурным элементом данной сети является остаточный блок,
математическое описание которого может быть представлено следующим
образом:

$$\mathbf{y} = F(\mathbf{x}, \{\mathbf{W}_i\}) + h(\mathbf{x}),$$ где
$F(\mathbf{x}, {\mathbf{W}_i})$ -- остаточную функцию; $h(\mathbf{x})$
-- функцию тождественного отображения или линейного проецирования.

Детализируя структуру остаточного блока, можно выразить
$F(\mathbf{x}, {\mathbf{W}_i})$ как композицию нескольких операций:

$$\begin{split}
        \mathbf{z}_1                    & = \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1), \\
        \mathbf{z}_2                    & = D(\mathbf{z}_1, p),                            \\
        \mathbf{z}_3                    & = \mathbf{W}_2\mathbf{z}_2 + \mathbf{b}_2,       \\
        F(\mathbf{x}, \{\mathbf{W}_i\}) & = \mathbf{z}_3.
    \end{split}$$ где
$\mathbf{W}_1, \mathbf{W}_2 \in \mathbb{R}^{m \times n}$ -- весовые
матрицы; $\mathbf{b}_1, \mathbf{b}_2 \in \mathbb{R}^m$ -- векторы
смещения; $\sigma(\cdot)$ -- функция активации ReLU; $D(\cdot, p)$ --
операция dropout с вероятностью $p$.

Функция активации ReLU определяется как [-@nair2010rectified]:
$$\sigma(x) = \max(0, x).$$

Механизм dropout представляет собой эффективный метод регуляризации
[-@srivastava2014dropout], широко применяемый в глубоких нейронных сетях
для предотвращения переобучения и повышения их обобщающей способности. В
контексте рассматриваемой суррогатной модели, dropout играет ключевую
роль в обеспечении надежности и устойчивости предсказаний.

Сущность метода dropout заключается в случайном \"выключении\"
определенной доли нейронов в процессе обучения. Математически этот
процесс может быть описан следующим образом:

$$\tilde{\mathbf{z}} = \mathbf{m} \odot \mathbf{z},$$ где
$\mathbf{z} \in \mathbb{R}^n$ -- вектор активаций нейронов;
$\mathbf{m} \in {0, 1}^n$ -- бинарная маска dropout;
$\tilde{\mathbf{z}}$ -- результирующий вектор после применения dropout.

Элементы маски $\mathbf{m}$ генерируются независимо из распределения
Бернулли с параметром $1-p$:

$$m_i \sim \text{Bernoulli}(1-p), \quad i = 1, \ldots, n,$$ где $p$ --
вероятность \"выключения\" нейрона, являющуюся одним из гиперпараметров
модели.

Значение $1-p$, соответственно, определяет вероятность сохранения
нейрона активным.

Применение dropout приводит к тому, что математическое ожидание
выходного значения каждого нейрона уменьшается в $1-p$ раз:

$$\mathbb{E}[\tilde{z}_i] = (1-p)z_i.$$

Для компенсации данного эффекта во время обучения, выход нейрона
корректируется путем масштабирования на $1/(1-p)$:

$$\tilde{\mathbf{z}} = \frac{1}{1-p}\mathbf{m} \odot \mathbf{z}.$$

Важно отметить, что на этапе инференса (применения обученной модели)
dropout не используется. Это эквивалентно использованию математического
ожидания активаций:

$$\mathbf{z}_{\text{test}} = \mathbb{E}[\tilde{\mathbf{z}}] = \mathbf{z}.$$

Применение dropout в остаточном блоке суррогатной модели может быть
выражено следующим образом:

$$\begin{split}
        \mathbf{z}_1         & = \sigma(\mathbf{W}_1\mathbf{x} + \mathbf{b}_1)                                             \\
        \tilde{\mathbf{z}}_1 & = \frac{1}{1-p}\mathbf{m} \odot \mathbf{z}_1, \quad \mathbf{m}_i \sim \text{Bernoulli}(1-p) \\
        \mathbf{z}_2         & = \mathbf{W}_2\tilde{\mathbf{z}}_1 + \mathbf{b}_2
    \end{split}$$ где $\sigma(\cdot)$ обозначает функцию активации;
$\mathbf{W}_1$ и $\mathbf{W}_2$ -- весовые матрицы; $\mathbf{b}_1$ и
$\mathbf{b}_2$ -- векторы смещения.

Остаточные блоки объединяются в последовательность, образуя глубокую
модель, способную захватывать сложные нелинейные зависимости между
входными параметрами и выходными метриками качества управления
электропневматическим приводом.

### Процесс обучения {#sec:ch4/sec3/subsec2}

### Оптимизация гиперпараметров {#sec:ch4/sec3/subsec3}

## Алгоритм построения фронта Парето {#sec:ch4/sec4}

### Генерация начальной выборки методом латинского гиперкуба {#sec:ch4/sec4/subsec1}

### Обучение ансамбля нейронных сетей {#sec:ch4/sec4/subsec2}

### Генерация и отбор Парето-оптимальных решений {#sec:ch4/sec4/subsec3}

## Визуализация и анализ фронта Парето {#sec:ch4/sec5}

### Методы визуализации многомерных фронтов Парето {#sec:ch4/sec5/subsec1}

### Метрики сравнения фронтов Парето {#sec:ch4/sec5/subsec2}

### Анализ чувствительности и робастности решений {#sec:ch4/sec5/subsec3}
